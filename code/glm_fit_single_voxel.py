import os
import argparse
import nibabel as nib
import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.model_selection import KFold
import glob
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import RidgeCV
import pickle
from scipy.stats import pearsonr

def save(file, name):
    with open(name, 'wb') as handle:
        pickle.dump(file, handle, protocol=pickle.HIGHEST_PROTOCOL)

def test_model_Ridge(X, y, n, saveto, save_results = True, shuffle=False, prefix = ""):
    if shuffle: # note that shuffling might artificially increase the encoding scores. Default is non-shuffled. All the analyses now are w/o shuffling.
        kf = KFold(n_splits=n, shuffle=True, random_state = 0)
    else:
        kf = KFold(n_splits=n, shuffle=False)
    out_reg = []
    # ç”¨æ¥å­˜å‚¨æ¯æŠ˜çš„ç›¸å…³ç³»æ•°(r)
    out_coefs = []
    # ç”¨æ¥å­˜å‚¨æ¯æŠ˜å­¦åˆ°çš„å›å½’ç³»æ•°ï¼ˆé•¿åº¦åº”ä¸º 300ï¼Œå› ä¸ºç‰¹å¾æ•°æ˜¯ 300)
    out_pred = []; y_tot = []
    # ç”¨æ¥æ”¶é›†æ‰€æœ‰æŠ˜çš„é¢„æµ‹å€¼ï¼ˆæµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ï¼‰ï¼Œæœ€ç»ˆåˆå¹¶ä¸€èµ·ç®—æ•´ä½“ Pearson ç›¸å…³ã€‚ æ”¶é›†æ‰€æœ‰æŠ˜çš„çœŸå®å€¼ã€‚
    X_scaler = StandardScaler()
    y_scaler = StandardScaler()
    # X_scaler / y_scaler: åˆ†åˆ«ç”¨äºå¯¹ X å’Œ y åš StandardScaler(å‡å€¼ä¸º 0ã€æ ‡å‡†å·®ä¸º 1)çš„å˜æ¢å¯¹è±¡ã€‚
    for train_index, test_index in tqdm(kf.split(X), total=n):
        # Normalizing embedding and responses. The normalization parameters are always estimated on the training data and transferred to the test data
        X_train = X_scaler.fit_transform(X[train_index])
        # X[train_index]: ({train_size}, 300)
        # è°ƒç”¨ fit_transform ä¼šï¼š å…ˆæ ¹æ® X_train çš„å‡å€¼å’Œæ ‡å‡†å·®åšæ‹Ÿåˆã€‚ å†å¯¹ X_train åšæ ‡å‡†åŒ–ï¼ˆæ¯ä¸ªç‰¹å¾å‡å»å‡å€¼ï¼Œé™¤ä»¥æ ‡å‡†å·®ï¼‰ã€‚
        # X_train è¿™é‡Œç”¨çš„æ˜¯åŒæ ·çš„å‡å€¼ã€æ ‡å‡†å·®ï¼ˆæ¥è‡ªè®­ç»ƒé›†çš„fitï¼‰ï¼Œå½¢çŠ¶ ({test_size}, 300)ã€‚
        X_test = X_scaler.transform(X[test_index])
        y_train = y_scaler.fit_transform(y[train_index].reshape(-1, 1)).flatten()
        # y[train_index]: åŸæœ¬æ˜¯ ({train_size},)ï¼Œé€šè¿‡ reshape(-1,1) å˜æˆ ({train_size}, 1)ï¼Œæ–¹ä¾¿ StandardScaler ä¸€æ¬¡æ€§å¤„ç†å•åˆ—æ•°æ®
        # æœ€ç»ˆ y_train å’Œ y_test éƒ½æ˜¯ä¸€ç»´å‘é‡ï¼Œå½¢çŠ¶åˆ†åˆ«ä¸º ({train_size},) å’Œ ({test_size},)
        y_test = y_scaler.transform(y[test_index].reshape(-1, 1)).flatten()
        reg = RidgeCV(alphas=0.1)
        #reg = RidgeCV(alphas=(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000)) # Alpha values log spaced. Alpha chosen with leave-one-out nested CV
        reg.fit(X_train, y_train)
        y_pred = reg.predict(X_test)
        #({test_size},)
        r, _ = pearsonr(y_test, y_pred)
        out_pred.extend(y_pred.tolist())
        # æœ¬æŠ˜çš„é¢„æµ‹ç»“æœåˆå¹¶è¿›ä¸€ä¸ªå…¨å±€åˆ—è¡¨ out_pred
        y_tot.extend(y_test.tolist())
        # æŠŠçœŸå®å€¼ä¹Ÿå­˜è¿›ä¸€ä¸ªå…¨å±€åˆ—è¡¨ y_tot
        coefs = reg.coef_#; print(coefs)
        # å›å½’ç³»æ•° (300,)ï¼Œå­˜åˆ° out_coefs é‡Œ
        out_coefs.append(coefs)
        out_reg.append(r)
        #æŠŠè¯¥æŠ˜çš„ Pearson ğ‘Ÿ æ”¾è¿›åˆ—è¡¨
    #print(round(np.mean(out_reg), 4))
    r_tot = pearsonr(out_pred, y_tot)[0]
    print(round(r_tot, 4))
    out_predictions = [y_tot, out_pred]
    if save_results:
        save(r_tot, f"results/rs/{prefix}{saveto}")
        save(out_reg, f"results/out_reg/{prefix}{saveto}") # saving all rs and coefficients for later use
        save(out_coefs, f"results/coefficients/{prefix}{saveto}")
        save(out_predictions, f"results/predictions/{prefix}{saveto}")
    return r_tot

# KæŠ˜åˆ’åˆ†ï¼šæŠŠè¯¥ voxel å¯¹åº”çš„ BOLD ä¿¡å· ğ‘¦
# y åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
# å¯¹ X å’Œ y å‡åšæ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰ï¼Œç„¶åç”¨ RidgeCV æ‹Ÿåˆã€‚
# æŠŠæ¯æŠ˜æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹æ‹¼æ¥èµ·æ¥(out_pred)å’ŒçœŸå®å€¼(y_tot)æ‹¼æ¥èµ·æ¥ã€‚
# åœ¨æ‰€æœ‰æŠ˜å…¨éƒ¨è·‘å®Œåï¼Œç›´æ¥ç”¨ pearsonr(out_pred, y_tot) å¾—åˆ°ä¸€ä¸ªæ•´ä½“çš„ç›¸å…³ç³»æ•°ã€‚


def main():
    parser = argparse.ArgumentParser(description='å…¨è„‘ç¼–ç åˆ†æ')
    parser.add_argument('--hrf_csv', type=str, required=True,
                      help='HRFæ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--derivatives_dir', type=str, required=True,
                      help='BOLDæ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--output_dir', type=str, required=True,
                      help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--n_splits', type=int, default=5,
                      help='KFoldæŠ˜æ•°, é»˜è®¤ä¸º5')
    parser.add_argument('--output_nii', action='store_true',
                    help='å¦‚æŒ‡å®šæ­¤å‚æ•°ï¼Œåˆ™è¾“å‡ºnii.gzç»“æœæ–‡ä»¶')
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)

    # ä»è·¯å¾„æå–è¯­è¨€ä»£ç 
    lang = os.path.normpath(args.hrf_csv).split(os.sep)[-2]
    hrf_model = os.path.basename(args.hrf_csv).split('_')[0]

    # æŸ¥æ‰¾åŒ¹é…çš„è¢«è¯•
    subject_dirs = glob.glob(os.path.join(args.derivatives_dir, f'sub-{lang}*'))
    subjects = [os.path.basename(d) for d in subject_dirs if os.path.isdir(d)]
    
    if not subjects:
        raise ValueError(f"æœªæ‰¾åˆ°{lang}è¯­è¨€çš„è¢«è¯•æ•°æ®")

    # åŠ è½½è®¾è®¡çŸ©é˜µ
    X = pd.read_csv(args.hrf_csv, header=None).values
    n_timepoints = X.shape[0]


    # ç°åœ¨æ›¿æ¢ä¸ºé«˜æ–¯å™ªå£°ï¼ŒåŒæ ·ä¿ç•™æ—¶é—´ç‚¹æ•°é‡
    # ä¸ºäº†æ¼”ç¤ºï¼Œå…ˆè¯»å–æ–‡ä»¶ï¼Œä»…è·å–å½¢çŠ¶ä¿¡æ¯ï¼š
    # X_shape = pd.read_csv(args.hrf_csv, header=None).values.shape
    # n_timepoints = X_shape[0]

    # ç”Ÿæˆä¸åŸ HRF ç›¸åŒå½¢çŠ¶çš„é«˜æ–¯å™ªå£°
    # loc=0, scale=1 ä¸ºå‡å€¼ 0ã€æ ‡å‡†å·® 1ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´
    # X = np.random.normal(loc=0.0, scale=1.0, size=X_shape)

    results = []

    for sub_id in tqdm(subjects, desc='å¤„ç†è¢«è¯•'):
        # æŸ¥æ‰¾BOLDæ–‡ä»¶
        # bold_pattern = os.path.join(args.derivatives_dir, sub_id, 'func',
        #                             f'{sub_id}_task-lpp{lang}_mergedTP-*_bold.nii.gz')
        bold_pattern = os.path.join(args.derivatives_dir, sub_id, 'func',
                                    f'{sub_id}_task-lpp*_mergedTP-*_bold.nii.gz')
        bold_files = glob.glob(bold_pattern)
        
        if not bold_files:
            print(f"\nè·³è¿‡{sub_id}: æœªæ‰¾åˆ°BOLDæ–‡ä»¶")
            continue
            
        try:
            bold_img = nib.load(bold_files[0])
            bold_data = bold_img.get_fdata()
            #(73, 90, 74, 2816)
        except Exception as e:
            print(f"\n{sub_id}åŠ è½½BOLDæ•°æ®å¤±è´¥: {str(e)}")
            continue

        # éªŒè¯æ—¶é—´ç»´åº¦
        if bold_data.shape[-1] != n_timepoints:
            print(f"\nè·³è¿‡{sub_id}: æ—¶é—´ç‚¹ä¸åŒ¹é…")
            continue

        # å‡†å¤‡å…¨è„‘æ•°æ®
        brain_shape = bold_data.shape[:-1]# (73, 90, 74)
        r_map = np.zeros(brain_shape, dtype=np.float32) * np.nan  # åˆå§‹åŒ– NaN

        # å°† BOLD reshape ä¸º (n_voxels, T)
        n_voxels = np.prod(brain_shape)
        bold_2d = bold_data.reshape(n_voxels, n_timepoints)

        # (5) å¾ªç¯æ¯ä¸ªvoxel, è°ƒç”¨ test_model_Ridge(X, y, n_splits, ...)
        #    yå°±æ˜¯è¯¥voxelçš„æ—¶é—´åºåˆ—
        for vox_idx in tqdm(range(n_voxels), desc="voxel-wise Ridge"):
            y = bold_2d[vox_idx, :]  # shape (2816, )
            print("voxel", vox_idx, " std of y =", np.std(y))

            
            # è°ƒç”¨ test_model_Ridge 
            #   - n_splits=args.n_splits
            #   - è¿™é‡Œæˆ‘ä»¬ä¼ ä¸€ä¸ªæ–‡ä»¶åå¯éšæ„, å‡è®¾"voxelXXXX"ä¹‹ç±», ä¹Ÿå¯ä¸ä¿å­˜
            #   - save_results=False é¿å…äº§ç”Ÿå¤§é‡æ–‡ä»¶
            r_tot = test_model_Ridge(X, y, args.n_splits, 
                                    saveto=f"voxel_{vox_idx}", 
                                    save_results=False, 
                                    shuffle=False)


            # å°†è¿™ä¸ªvoxelçš„ç›¸å…³å€¼å­˜åˆ° r_map é‡Œ
            r_map.flat[vox_idx] = r_tot
        
        # (6) å…¨è„‘å¹³å‡rå€¼
        #     æ³¨æ„ï¼šå¦‚æœæŸäº›voxelå€¼æ˜¯NaNæˆ–æ— æ•ˆï¼Œå¯ä»¥ç”¨ ~np.isnan(r_map) åšç­›é€‰
        brain_mean_r = np.nanmean(r_map)
        print(sub_id,", Whole brain average r:", brain_mean_r)
        results.append(brain_mean_r)


        if args.output_nii:
            out_nii = nib.Nifti1Image(r_map, affine=bold_img.affine)
            out_path = os.path.join(args.output_dir, f"{sub_id}_voxelwise_r.nii.gz")
            nib.save(out_nii, out_path)
            print("Saved voxelwise r-map to:", out_path)

        csv_path = os.path.join(args.output_dir, f"{sub_id}_rmap_summary.csv")
        pd.DataFrame({"voxel_mean_r": [brain_mean_r]}).to_csv(csv_path, index=False)


    # ç”Ÿæˆæ±‡æ€»ç»“æœ
    if results:
        df = pd.DataFrame(results, columns=["mean_pearson_r"])
        df.insert(0, "subject", subjects)  # æ·»åŠ è¢«è¯• ID åˆ—
        grand_avg = df["mean_pearson_r"].mean()

        # æ·»åŠ æ•´ä½“å¹³å‡å€¼
        df = pd.concat([df, pd.DataFrame([{
            "subject": "average",
            "mean_pearson_r": grand_avg,
        }])], ignore_index=True)
        
        os.makedirs(args.output_dir, exist_ok=True)
        output_file = os.path.join(args.output_dir, f"{hrf_model}_{lang}_results.csv")
        df.to_csv(output_file, index=False)
        print(f"\nåˆ†æå®Œæˆï¼Œç»“æœä¿å­˜è‡³: {output_file}")
    else:
        print("\næœªå¤„ç†ä»»ä½•è¢«è¯•æ•°æ®")


if __name__ == "__main__":
    main()